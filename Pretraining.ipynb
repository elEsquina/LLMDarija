{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d21d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\um6p\\anaconda3\\envs\\llmtest\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\um6p\\anaconda3\\envs\\llmtest\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\um6p\\anaconda3\\envs\\llmtest\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\um6p\\anaconda3\\envs\\llmtest\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\um6p\\anaconda3\\envs\\llmtest\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U torch datasets transformers[torch] wandb -q\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a543026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, AutoConfig, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51455174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الوالي الزاز كود العيون\\n \\nتلقى الرئيس الجزائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الوالي الزاز كود العيون\\n \\nبدا مستشار الرئيس ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>أنس العمري كود\\nحذرت المملكة المتحدة مواطنيها ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كود الرباط\\nالسياسة فكولشي  من الرياضة للثقافة...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>سعيد الشاوي كود\\nالعطلة الصيفية 2025 مزال مستم...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  الوالي الزاز كود العيون\\n \\nتلقى الرئيس الجزائ...\n",
       "1  الوالي الزاز كود العيون\\n \\nبدا مستشار الرئيس ...\n",
       "2  أنس العمري كود\\nحذرت المملكة المتحدة مواطنيها ...\n",
       "3  كود الرباط\\nالسياسة فكولشي  من الرياضة للثقافة...\n",
       "4  سعيد الشاوي كود\\nالعطلة الصيفية 2025 مزال مستم..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('assets/data/articles_cleaned.csv')[['content']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3594da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(x_train),\n",
    "    'test': Dataset.from_pandas(x_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8561f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49152\n",
      "8192\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}\n"
     ]
    }
   ],
   "source": [
    "# check the vocabe size of our tokenizer\n",
    "print(f\"{len(tokenizer)}\")\n",
    "# model max length (means the max len of the input)\n",
    "print(f\"{tokenizer.model_max_length}\")\n",
    "# tokenizer special tokens\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc72ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20052, 332, 16143, 263, 373, 1483, 85]\n",
      "['Sal', 'am', 'Ġana', 'Ġo', 'th', 'man', 'e']\n",
      "Salam ana othmane\n"
     ]
    }
   ],
   "source": [
    "example=\"Salam ana othmane\"\n",
    "ids=tokenizer.encode(example)\n",
    "print(ids)\n",
    "tokens=tokenizer.convert_ids_to_tokens(ids)\n",
    "print(tokens)\n",
    "decode_=tokenizer.decode(ids)\n",
    "print(decode_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd53643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples, context_length=128):\n",
    "  results=tokenizer(\n",
    "      examples[\"content\"],\n",
    "      truncation=True,\n",
    "      max_length=context_length,\n",
    "      return_overflowing_tokens=True, # with this you will get also the input ids with length less than context_length\n",
    "      return_length=True\n",
    "  )\n",
    "  input_batch=[]\n",
    "  for l,in_ids in zip(results[\"length\"],results[\"input_ids\"]):\n",
    "    if l==context_length:\n",
    "      input_batch.append(in_ids)\n",
    "  return {\"input_ids\":input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ceea8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8143/8143 [00:12<00:00, 633.54 examples/s]\n",
      "Map: 100%|██████████| 2036/2036 [00:03<00:00, 619.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize,batched=True,remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ddc84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UM6P\\AppData\\Local\\Temp\\ipykernel_21112\\564552371.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"test_dir\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    logging_steps=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a89019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mo-azoubi-machacile\u001b[0m (\u001b[33mo-azoubi-machacile-mohammed-v-university-in-rabat\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\UM6P\\Desktop\\Side Projects\\DarijaLLM\\wandb\\run-20250731_151616-afnckdqj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/o-azoubi-machacile-mohammed-v-university-in-rabat/huggingface/runs/afnckdqj' target=\"_blank\">test_dir</a></strong> to <a href='https://wandb.ai/o-azoubi-machacile-mohammed-v-university-in-rabat/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/o-azoubi-machacile-mohammed-v-university-in-rabat/huggingface' target=\"_blank\">https://wandb.ai/o-azoubi-machacile-mohammed-v-university-in-rabat/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/o-azoubi-machacile-mohammed-v-university-in-rabat/huggingface/runs/afnckdqj' target=\"_blank\">https://wandb.ai/o-azoubi-machacile-mohammed-v-university-in-rabat/huggingface/runs/afnckdqj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UM6P\\anaconda3\\envs\\LLMTest\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='16770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/16770 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
